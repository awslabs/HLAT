"""
Utility functions for working with HF datasets
"""
import os
import logging
from typing import List, Union, Dict
import numpy as np
import pyarrow as pa
import datasets

# don't rely on cached index files generated by shuffling
datasets.disable_caching()

# fixed seed used to shuffle before sharding
SHUFFLE_SEED = 1234


def build_dataset_from_filepaths(
    paths: list,
    sampling_probs: Union[None, List[float]] = None,
    shard_idx: int = None,
    num_shards: int = None,
    renormalize: bool = False,
    hf_kwargs: Dict = None,
    key: str = "text",
    rename_dataset_key: bool = False,
) -> datasets.arrow_dataset.Dataset:
    """
    Wrapped to construct a HuggingFace dataset from a list of filepaths.
    Checks that each filepath points to a single-chunk arrow file.
    Args:
        paths: arrow files that will be concatenated
        sampling_probs: How much to weigh each per-arrow file. Concatenate if None
        shard_idx: which shard index to collect
        num_shards: number of shards to split each dataset into
        renormalize: if True, renormalize sampling_probs to sum to 1
        hf_kwargs: kwargs to pass to data mixing function
        key: which key to use to check for single-chunk arrow files
        rename_dataset_key: If the existing dataset does not have the appropriate key,
                            rename the dataset column name to match the key name passed above
    """
    # TODO(@colehawk): benchmark .take performance as number of files increases
    if len(paths) > 1:
        logging.warning(
            "Concatenating arrow datasets can slow down .take()."
            "Make sure to use pre-fetch."
        )

    for path in paths:
        check_arrow_for_single_chunk(path, key=key, use_existing_key=rename_dataset_key)

    # load dataset subcomponents
    arrow_datasets = [datasets.load_from_disk(x) if os.path.isdir(x)
                      else datasets.arrow_dataset.Dataset.from_file(x) for x in paths]

    #If the existing dataset does not have the appropriate key, rename the first column name to match the key
    if rename_dataset_key:
        arrow_datasets = [x.rename_column(x.column_names[0], key) if key not in x.column_names else x for x in arrow_datasets]

    if num_shards and num_shards > 0:
        logging.info("Sharding huggingface component datasets")
        arrow_datasets = [
            x.shard(index=shard_idx, num_shards=num_shards) for x in arrow_datasets
        ]

    # TODO: Mixing, concatenating datasets when some of them having missing keys/columns leads to
    # a combined dataset with all the column names and the missing entries assigned a value of None.
    if sampling_probs:
        num_sampling_probs = len(sampling_probs)
        num_paths = len(paths)
        assert (
            num_sampling_probs == num_paths
        ), f"Got {num_sampling_probs} sampling probs but {num_paths} paths"
        logging.info("Mixing component datasets")

        # shuffle before mixing since mixing will
        # read from the first per-dataset index and increment
        arrow_datasets = [x.shuffle(seed=SHUFFLE_SEED) for x in arrow_datasets]
        concatenated_dataset = mix_datasets(
            dataset_list=arrow_datasets,
            sampling_probs=sampling_probs,
            renormalize=renormalize,
            **hf_kwargs,
        )
    else:
        concatenated_dataset = datasets.concatenate_datasets(arrow_datasets)

    # final shuffle to ensure randomness after
    # mix_datasets and concatenation
    concatenated_dataset = concatenated_dataset.shuffle(seed=SHUFFLE_SEED)

    return concatenated_dataset


def check_arrow_for_single_chunk(filepath: str, key: str = "text", use_existing_key: bool = False) -> None:
    """
    Checks a .arrow file to ensure it's single-chunk
    Args:
        filepath: file to check for single-chunk
        key: column key to check
        use_existing_key: If the passed key above is not in the existing keys, use the existing key
                          for the check
    """
    mmap = pa.memory_map(filepath)

    # Check table has single chunk
    # https://issues.apache.org/jira/browse/ARROW-11989
    stream = pa.ipc.open_stream(mmap).read_all()

    #If key passed above is not in the existing keys, use the existing key for the check
    if use_existing_key:
        existing_keys = stream.schema.names #Returns a list of existing key names
        if key not in existing_keys:
            key = existing_keys[0] #Pick the first key

    assert (
        len(stream[key].chunks) == 1
    ), f"File {filepath} is not a single-chunk arrow file"


def check_min_sampling_prob(sampling_probs: List[float]) -> None:
    """
    Checks that all sampling probabilities are larger than the required minimum.
    Very low sampling rates can lead to large index building times for HF datasets.
    Args:
        sampling_probs: mixing proportions
    """
    MIN_SAMPLING_PROB = 0.1
    if min(sampling_probs) < MIN_SAMPLING_PROB:
        logging.warning(
            "Mixing with low sampling probabilities leads to long iterator construction time"
        )


def mix_datasets(
    dataset_list: List[datasets.arrow_dataset.Dataset],
    sampling_probs: List[float],
    renormalize: bool = False,
    **hf_kwargs,
) -> datasets.arrow_dataset.Dataset:
    """
    Mix a list of datasets with a given set of sampling probabilities. Non-deterministic unless seeded.
    Args:
        dataset_list: huggingface datasets to mix
        sampling_probs: mixing proportion per-dataset, must sum to 1 unless renormalize=True
        renormalize: if True, renormalize sampling_probs to 1
        hf_kwargs: keyword args for hugginface interleave_datasets, include seed here
    """
    logging.warning("Mixing HF datasets leads to long dataset creation time")

    if renormalize:
        normalization_const = sum(sampling_probs)
        sampling_probs = [x / normalization_const for x in sampling_probs]

    check_min_sampling_prob(sampling_probs)

    assert np.allclose(
        sum(sampling_probs), 1.0
    ), f"Sampling probs must sum to 1, got {sum(sampling_probs)}"

    dataset = datasets.interleave_datasets(
        dataset_list, probabilities=sampling_probs, **hf_kwargs
    )

    return dataset
